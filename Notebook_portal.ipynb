{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jovian: configure and start analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Preparation\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add all raw Illumina PE data to a folder named \"raw_data\" relative from the location of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add samplesheet script of Mark, list it below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add pipeline config file, list it below. Add a link to the edit page?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load profile/pipeline_parameters.yaml\n",
    "######################################\n",
    "### General parameters             ###\n",
    "######################################\n",
    "###### GOTCHA WARNING\n",
    "### Only use spaces in this file, NO tabs!\n",
    "\n",
    "threads: # It's advisible efficiciency-wise to set these numbers only on a log2 scale (e.g. 2,4,8,16)\n",
    "    Clean_the_data: 2\n",
    "    HuGo_removal_pt1_alignment: 4\n",
    "    HuGo_removal_pt2_Sam2Bam: 4\n",
    "    HuGo_removal_pt4_extract_paired_unmapped_reads: 4\n",
    "    HuGo_removal_pt5_extract_unpaired_unmapped_reads: 4\n",
    "    De_novo_assembly: 12\n",
    "    Taxonomic_classification_of_scaffolds: 12   # BLAST accepts a maximum of 16 threads, any value  >16 results in error\n",
    "    Generate_contigs_metrics_pt1_alignment_of_reads_to_scaffolds: 4\n",
    "    Generate_contigs_metrics_pt2_Sam2Bam: 4\n",
    "    SNP_calling_pt1_calling: 12\n",
    "    Fragment_length_analysis_pt2_alignment: 4\n",
    "    quantify_output: 8 # For reading lengths of fastq files in parallel; WARNING: this is a local rule!\n",
    "\n",
    "databases:                                                                                                               # Location of NT, NR and TaxDB are stored via an alias in your home directory (~/.ncbirc). They should work with keywords \"nt\" \"nr\" \"taxdb\"\n",
    "    HuGo_ref: /mnt/db/Reference_genomes/Homo_sapiens/NCBI/GRCh38/Sequence/Bowtie2Index_without_EBV_virus_chr/genome.fa   # Location of HuGo assembly GRCH38 from which I manually removed the EBV chromosome, see the README.md for explanation  \n",
    "    Krona_taxonomy: /mnt/db/taxonomy_krona/                                                                              # Location of the Krona taxonomy files, these are updated weekly via Crontab (thanks to Robert)\n",
    "    virusHostDB: /mnt/db/Virus-Host_interaction_DB/virushostdb.tsv                                                       # Location of the Virus-Host interaction database, see their publication at .... TODO\n",
    "    NCBI_new_taxdump_rankedlineage: /mnt/db/new_taxdump/rankedlineage.dmp.delim                                          # Location of the NCBI's \"new_taxdump\" file \"rankedlineage.dmp\", see their website at ...... TODO\n",
    "    NCBI_new_taxdump_host: /mnt/db/new_taxdump/host.dmp.delim                                                            # Location of the NCBI's \"new_taxdump\" file \"host.dmp\", see their website at ....... TODO\n",
    "\n",
    "typingtool_http:   #TEMP, remove this comment later: http://mpftypl01-ext-p.rivm.nl:8080 --> internal link (bypasses the RIVM proxy and thus the DDOS protection)\n",
    "    NoV_TT_http: https://www.rivm.nl/mpf/typingservice/norovirus          # Link to the NoV typingtool (RIVM mirror), the webversion is accessible via https://www.rivm.nl/mpf/typingtool/norovirus\n",
    "    EV_TT_http: https://www.rivm.nl/mpf/typingservice/enterovirus         # Link to the EV typingtool (RIVM mirror), the webversion is accessible via https://www.rivm.nl/mpf/typingtool/enterovirus\n",
    "    test3_TT_http: https://www.rivm.nl/mpf/typingservice/test3            # Link to the \"test3\" typingtool (RIVM mirror), the webversion is accessible via https://www.rivm.nl/mpf/typingtool/test3\n",
    "\n",
    "server_info:\n",
    "    http_adress: http://rivm-biohn-l01p.rivm.ssc-campus.nl\n",
    "    port: 8080\n",
    "\n",
    "HTML_index_titles:\n",
    "    IGVjs_title: \"Interactive genome viewers for all samples in this run:\"\n",
    "    heatmap_title: \"Interactive heatmaps of different taxonomic levels for all samples in this run:\"\n",
    "\n",
    "######################################\n",
    "### Software parameters            ###\n",
    "######################################\n",
    "###### GOTCHA WARNING\n",
    "### Only use spaces in this file, NO tabs!\n",
    "\n",
    "Trimmomatic:                                                                                                    # http://www.usadellab.org/cms/?page=trimmomatic\n",
    "    adapter_removal_config: ILLUMINACLIP:files/trimmomatic_0.36_adapters_lists/NexteraPE-PE.fa:2:30:10:8:true   # For the Nextera PE lib prep adapters\n",
    "    quality_trimming_config: SLIDINGWINDOW:5:30                                                                 # Default: 5 nucleotides window size, minimum average Phred score of 30\n",
    "    minimum_length_config: MINLEN:50                                                                            # Default: Remove anything smaller than 50 nucleotides\n",
    "    \n",
    "HuGo_removal:\n",
    "    alignment_type: --local  # Choose if you want to do \"local\" or \"global\" alignment. Keep this as \"--local\" unless you know what you're doing.\n",
    "\n",
    "SPAdes:                             # TODO: Latest version of SPAdes does this automically, need to update to new version first. See http://cab.spbu.ru/files/release3.12.0/manual.html section \"Multi-cell data set with read lengths 2 x 250\"\n",
    "    kmersizes: 21,33,55,77         # uncomment this line if you want to use the pipeline for short Illumina reads (<250 nt in length). Then alos comment the line below.\n",
    "#    kmersizes: 21,33,55,77,99,127   # uncomment this line if you want to use the pipeline for longer Illumina reads (>250 nt in length). Then also comment the line above.\n",
    "\n",
    "scaffold_minLen_filter:\n",
    "    minlen: 500             # Minimum allowed scaffold size to be allowed for downstream processessing. Advice, use a minimum length that is atleast 1nt greater than your Illumina read length\n",
    "\n",
    "taxonomic_classification:   # NT database is hardcoded for now\n",
    "    evalue: 0.05            # E-value threshold for saving hits\n",
    "    max_target_seqs: 10     # Maximum number of target sequences to report\n",
    "\n",
    "taxonomic_classification_LCA:\n",
    "    bitscoreDeltaLCA: 5   # Every taxonomic hit within this bitscore distance will be used in the LCA analysis.\n",
    "\n",
    "SNP_calling:\n",
    "    max_cov: 20000     # See comments in the Snakefile rule \"SNP_calling_pt1_calling\" for explanation. Don't change this value unless you know what you're doing.\n",
    "    minimum_AF: 0.05   # This is the minimum allelle frequency for which you want a SNP to be reported. Default is 5%.\n",
    "\n",
    "ORF_prediction:\n",
    "    procedure: meta      # Set the prodigal procedure. Use \"meta\" for metagenomics data.\n",
    "    output_format: gff   # Set the ORF prediction output format. The format that IGVjs (the alignment viewer program) accepts is \"gff\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add snakemake configuration and cluster configuration file below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-08T16:22:54.106611Z",
     "start_time": "2019-03-08T16:22:54.093337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "\tContents of the config.yaml (contains the Snakemake CLI parameters):\n",
      "________________________________________________________________________________\n",
      "### Profiles parameters: (See: https://snakemake.readthedocs.io/en/stable/executable.html?highlight=profile#profiles AND https://github.com/snakemake-profiles/doc )\n",
      "#dryrun: false\n",
      "#verbose: false\n",
      "#printshellcmds: true\n",
      "#quiet: false\n",
      "#reason: true\n",
      "#keep-going: false\n",
      "notemp: true\n",
      "cores: 120\n",
      "resources:\n",
      "    TT_connections=3\n",
      "latency-wait: 60\n",
      "local-cores: 4\n",
      "use-conda: true\n",
      "drmaa: \" -q bio-prio -n {threads} -R \\\"span[hosts=1]\\\"\"\n",
      "drmaa-log-dir: logs/drmaa\n",
      "jobname: PZN_{name}.jobid{jobid}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo -e \"________________________________________________________________________________\\n\\tContents of the config.yaml (contains the Snakemake CLI parameters):\\n________________________________________________________________________________\"\n",
    "cat profile/config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add link to demultiplexing notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embed tree HTML of all the log files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the snakemake start code to the cell below. NB, quiet mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to the report notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add Jovian logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "______________\n",
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N.B. This can probably be automated later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First move to the parentfolder where you want to do your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /data     # Replace the \"/data\" with the path where you want to go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, pull the latest code from Gitlab via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode, still need to check\n",
    "git clone http://rivm-git/PZN.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename the newly created folder to the name of your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode, still need to check\n",
    "mv PZN_name [my_analysis_name]\n",
    "mkdir [my_analysis_name]/raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, move the files you want to analyse to your analysis folder and in the subfolder named \"raw_data\". N.B. they need to be paired Illumina fastq's in order to work properly. Use an FTP-program for this, e.g. Filezilla."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Work in progress, pseudocode\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### N.B. The parts below are doable, but will take a lot of time to implement. I've postponed it for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit your sample sheet:\n",
    "\n",
    "This would allow specifying file locations anywhere on the filesystem, and is required to accurately work with metadata file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for jupyter editor of sample sheet\n",
    "# Layout:\n",
    "#    Sample_name    Sample_path_R1                                 Sample_path_R1\n",
    "#    sample_1       /data/location/to/my/data/sample_1_R1.fastq    /data/location/to/my/data/sample_1_R2.fastq\n",
    "#    sample_2       /data/location/to/my/data/sample_2_R1.fastq    /data/location/to/my/data/sample_2_R2.fastq\n",
    "#    sample_etc     /data/location/to/my/data/sample_etc_R1.fastq  /data/location/to/my/data/sample_etc_R2.fastq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit your metatdata sheet:\n",
    "\n",
    "This would allow sample-swap heuristics, different procedures based on metadata (e.g. different trimming for PE or SE data), and provides more clear questions and logging --> tracability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for jupyter editor of metadata sheet\n",
    "# Layout:\n",
    "#    Sample_name Matrix       Host     Gender    Seq_and_lib    Goal\n",
    "#    sample_1    serum        Human    Female    Illumina PE    metagenomics\n",
    "#    sample_2    feces        Human    Male      Illumina SE    metagenomics\n",
    "#    sample_etc  cell-cult    Chimp    NA        Nanopore       Quasispecies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Start your analysis\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to see if there are any missing files or errors in the Snakefile. This is called a \"dry-run\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source activate PZN\n",
    "snakemake -npj20 -s Snakefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are no errors in the dry-run, perform the actual analysis via the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source activate PZN\n",
    "snakemake -pj20 -s Snakefile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Work in progress\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, the pipeline expects that the input files are stored in a folder named \"raw_data\". Also, it expects files are unpacked and are named as [sample_name]_R[1|2].fastq. If this is not the case, perform these commands first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the files in folder raw_data using \"parallel\" to use full compute power\n",
    "\n",
    "cd raw_data/\n",
    "parallel gunzip ::: *.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To rename the files to an accepted format\n",
    "\n",
    "# First by eye determine what is listed between the \"_R[1|2]\" and \".fastq\". I.e. \"_0001\". Then insert that string into the code below.\n",
    "\n",
    "for file in raw_data/*.fastq\n",
    "do\n",
    "    mv $file ${file/_0001/}\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "- Fix bug in bin/determine_GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATA = \"raw_data_test/\"          # Here you set the input folder containing the input PE fastq's\n",
    "\n",
    "#from os.path import join\n",
    "#(SAMPLES, DELME,) = glob_wildcards(join(RAW_DATA, \"{sample,[^/]+}{delme,_R[1|2]}.fastq\"))\n",
    "\n",
    "import glob\n",
    "from os.path import join\n",
    "FILES= glob.glob(join(RAW_DATA, \"*.fastq\")) # Glob all files (irrespective of R1 or R2)\n",
    "FILES_R1= glob.glob(join(RAW_DATA, \"*R1*.fastq\")) # Glob all R1 files\n",
    "FILES_R2= glob.glob(join(RAW_DATA, \"*R2*.fastq\")) # Glob all R2 files\n",
    "SAMPLES = [i.split(\"/\")[-1].split(\"_R1\")[0] for i in files_R1] # Create sample basenames\n",
    "SAMPLES_R1 = [i.split(\"/\")[-1].split(\"_R1\")[0] + \"_R1\" for i in files_R1] # Create sample basenames\n",
    "SAMPLES_R2 = [i.split(\"/\")[-1].split(\"_R2\")[0] + \"_R2\" for i in files_R2] # Create sample basenames\n",
    "\n",
    "\n",
    "print(FILES)\n",
    "print(FILES_R1)\n",
    "print(FILES_R2)\n",
    "print(SAMPLES)\n",
    "print(SAMPLES_R1)\n",
    "print(SAMPLES_R2)\n",
    "\n",
    "#samples_list = [i.split(\"_cov\")[0] for i in samples_list_cov]\n",
    "#basename_sample = sample.split(\"/\")[-1]   # Remove the path from sample, thus, leave only the basename (extension has been removed priorly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'7_20688_AGGCAGAAGCGTAAGA_L001', '9_20689_TCCTGAGCGCGTAAGA_L001'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "samples = set([ f.split(\"_R\")[0] for f in os.listdir(\"v2/raw_data\") if f.endswith(\".fastq\")])\n",
    "print(samples)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [conda env:Jovian_master] *",
   "language": "python",
   "name": "conda-env-Jovian_master-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
